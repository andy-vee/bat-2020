{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!usr/bin/python\n",
    "import pandas as pd, numpy as np\n",
    "# import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "# from ast import literal_eval\n",
    "import requests, datetime, time\n",
    "import json\n",
    "from listing_parsers import active_listing_parser, closed_listing_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bring in auctions html\n",
    "\n",
    "headers = {\n",
    "    'User-Agent': 'AV User Agent 20201011',\n",
    "    'From': 'anvance@gmail.com'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site = requests.get('https://bringatrailer.com/auctions', headers=headers)\n",
    "site.status_code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse active auctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Soup it\n",
    "soup = BeautifulSoup(site.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse and build tuples for a table\n",
    "\n",
    "labels = ['uid',\n",
    "          'listing_title', \n",
    "          'listing_excerpt', \n",
    "          'listing_link', \n",
    "          'latest_bid', \n",
    "          'year',\n",
    "          'make',\n",
    "          'model',\n",
    "          'category',\n",
    "          'keyword',\n",
    "          'vin',\n",
    "          'latitude',\n",
    "          'longitude']\n",
    "\n",
    "listing_tuples = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in soup.findAll('div', class_ = lambda x: (x == 'auctions-item-container'))[1:]:\n",
    "    try:\n",
    "        uid = item['data-update']    \n",
    "    except AttributeError:\n",
    "        uid = 'not found'\n",
    "    try:\n",
    "        listing_title = item.find('h3',class_='auctions-item-title').text    \n",
    "    except AttributeError:\n",
    "        listing_title = 'not found'\n",
    "    try:\n",
    "        listing_excerpt = item.find('div', class_='auctions-item-excerpt').text\n",
    "    except AttributeError:\n",
    "        listing_excerpt = 'not found'\n",
    "    try:\n",
    "        listing_link = item.find('h3').find('a', attrs = {'href': True})['href']\n",
    "    except AttributeError:\n",
    "        listing_link = 'not found'\n",
    "    try:\n",
    "        latest_bid = item.find_all('span', attrs= {'data-current': True})[0]['data-current']\n",
    "    except AttributeError:\n",
    "        latest_bid = 'not found'\n",
    "    try:\n",
    "        year = json.loads(item['data-searchable'])['year']\n",
    "    except AttributeError:\n",
    "        year = 'not found'\n",
    "    try:\n",
    "        make = json.loads(item['data-searchable'])['make']\n",
    "    except AttributeError:\n",
    "        make = 'not found'\n",
    "    try:\n",
    "        model = json.loads(item['data-searchable'])['model']\n",
    "    except AttributeError:\n",
    "        model = 'not found'\n",
    "    try:\n",
    "        category = json.loads(item['data-searchable'])['category']\n",
    "    except AttributeError:\n",
    "        category = 'not found'\n",
    "    try:\n",
    "        keyword = json.loads(item['data-searchable'])['keyword']\n",
    "    except AttributeError:\n",
    "        keyword = 'not found'\n",
    "    try:\n",
    "        vin = json.loads(item['data-searchable'])['vin']\n",
    "    except AttributeError:\n",
    "        vin = 'not found'\n",
    "    try:\n",
    "        latitude = item['data-lat']\n",
    "    except AttributeError:\n",
    "        latitude = 'not found'\n",
    "    try:\n",
    "        longitude = item['data-lon']\n",
    "    except AttributeError:\n",
    "        longitude = 'not found'\n",
    "    listing_tuple = (uid,\n",
    "                     listing_title, \n",
    "                     listing_excerpt, \n",
    "                     listing_link, \n",
    "                     latest_bid, \n",
    "                     year,\n",
    "                     make,\n",
    "                     model,\n",
    "                     category,\n",
    "                     keyword,\n",
    "                     vin,\n",
    "                     latitude,\n",
    "                     longitude)\n",
    "    listing_tuples.append(listing_tuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "staging_df = pd.DataFrame(listing_tuples, columns = labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_string = datetime.datetime.strftime(datetime.datetime.now(), '%Y-%m-%d_%H-%M-%S')\n",
    "time_string_2 = datetime.datetime.strftime(datetime.datetime.now(), '%Y-%m-%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "staging_df['scrape_ts'] = time_string_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extradeetslist = list()\n",
    "for url in staging_df['listing_link']:\n",
    "    dict_ = active_listing_parser(url)\n",
    "    dict_['url'] = url\n",
    "    random_number = np.abs(np.random.normal())*10\n",
    "    extradeetslist.append(dict_)\n",
    "    time.sleep(random_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extradeetsdf = pd.DataFrame(extradeetslist)\n",
    "staging_df = staging_df.merge(extradeetsdf, how='inner', left_on='listing_link',right_on='url')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del(staging_df['url'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "staging_df.to_csv('/home/ubuntu/projects/bat-2020-data/bat_listings_output_v2_%s.csv' % time_string, \n",
    "                  index=True, index_label='fid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse auction results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = requests.get('https://bringatrailer.com/auctions/results', headers=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_soup = BeautifulSoup(results.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['title','result', 'link']\n",
    "results_list = []\n",
    "for item in results_soup.find_all('div', class_='exceptional-item-extended'):\n",
    "    title = item.find('a').text\n",
    "    result = item.find('div', class_='exceptional-item-status').text\n",
    "    link = item.find('a')['href']\n",
    "    results_tuple = (title, result, link)\n",
    "    results_list.append(results_tuple)\n",
    "for item in results_soup.find_all('div', class_='auctions-item-extended'):\n",
    "    title = item.find('span').text\n",
    "    result = item.find('div', class_='auctions-item-status').text\n",
    "    link = item.find('a')['href']\n",
    "    results_tuple = (title, result, link)\n",
    "    results_list.append(results_tuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results_list, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extradeetslist = list()\n",
    "for url in results_df['link']:\n",
    "    dict_ = closed_listing_parser(url)\n",
    "    dict_['url'] = url\n",
    "    random_number = np.abs(np.random.normal())*10\n",
    "    extradeetslist.append(dict_)\n",
    "    time.sleep(random_number)\n",
    "extradeetsdf = pd.DataFrame(extradeetslist)\n",
    "results_df = results_df.merge(extradeetsdf, how='inner', left_on='link',right_on='url')\n",
    "del(results_df['url'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_string = datetime.datetime.strftime(datetime.datetime.now(), '%Y-%m-%d_%H-%M-%S')\n",
    "results_df.to_csv('/home/ubuntu/projects/bat-2020-data/bat_results_output_v2_%s.csv' % time_string, index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Scrape complete - uploading to S3...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S3 Upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
